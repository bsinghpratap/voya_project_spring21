{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from gensim.corpora import Dictionary\n",
    "from gensim.models.ldamodel import LdaModel\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import numpy as np\n",
    "import heapq\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, confusion_matrix, accuracy_score\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_type = \"sentence_lda\"\n",
    "base = \"/mnt/nfs/scratch1/hshukla/sentence_results/\"\n",
    "output_folder = \"/mnt/nfs/scratch1/hshukla/prediction_results/\"\n",
    "model_base = \"sen_lda_{}_{}.model\"\n",
    "if job_type == \"sentence_lda\":\n",
    "    window_size = 7\n",
    "else:\n",
    "    window_size = None\n",
    "\n",
    "# Settings\n",
    "lda_risk_path = base + model_base.format(\"item1a_risk\", window_size)\n",
    "lda_mda_path = base +  model_base.format(\"item7_mda\", window_size)\n",
    "data_path = base + \"df_sen_7_3.pkl\"\n",
    "output_folder_path = base + \"predictions/\"\n",
    "is_pkl = True\n",
    "is_corp_filter = True\n",
    "is_vanilla = job_type == \"vanilla_lda\"\n",
    "start_year = 2012\n",
    "end_year = 2015\n",
    "predict_year = 2016\n",
    "train_range = list(range(start_year,end_year+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "if is_pkl:\n",
    "    data = pd.read_pickle(data_path)\n",
    "else:\n",
    "    data = pd.read_csv(data_path)\n",
    "data = data.sort_values(by=['year_x'])\n",
    "lda_risk = LdaModel.load(lda_risk_path)\n",
    "lda_mda = LdaModel.load(lda_mda_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using sentence_lda model for [2012,2015] inclusive predicting for 2016\n",
      "# train rows: 5155\n",
      "# test rows: 1307\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-4-6087fe41bd48>:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data_valid[\"is_dps_cut\"] = data_valid[\"is_dps_cut\"].astype(int)\n"
     ]
    }
   ],
   "source": [
    "# Find subset of valid data\n",
    "print(\"Using {} model for [{},{}] inclusive predicting for {}\".format(job_type, start_year, end_year, predict_year))\n",
    "data[\"is_dividend_payer\"] = data[\"is_dividend_payer\"].astype(bool)\n",
    "data_valid = data[data[\"is_dividend_payer\"] & data[\"is_dps_cut\"].notnull()]\n",
    "data_valid[\"is_dps_cut\"] = data_valid[\"is_dps_cut\"].astype(int)\n",
    "\n",
    "# train/test\n",
    "data_train = data_valid[(data_valid.year_x >= start_year) & (data_valid.year_x <= end_year)]\n",
    "data_test = data_valid[data_valid.year_x == predict_year]\n",
    "print(\"# train rows: {}\".format(len(data_train)))\n",
    "print(\"# test rows: {}\".format(len(data_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The dictionary is defined over the entire training dataset\n",
    "if is_vanilla:\n",
    "    risk_docs = []\n",
    "    mda_docs = []\n",
    "else:\n",
    "    risk_docs = [sentence_grp for doc in data_train[\"item1a_risk\"].to_list() for sentence_grp in doc]\n",
    "    mda_docs = [sentence_grp for doc in data_train[\"item7_mda\"].to_list() for sentence_grp in doc]\n",
    "\n",
    "risk_dict = Dictionary(risk_docs)\n",
    "mda_dict = Dictionary(risk_docs)\n",
    "if is_corp_filter: # Used filtering in sent-lda to speed things up\n",
    "    risk_dict.filter_extremes(no_below=10)\n",
    "    mda_dict.filter_extremes(no_below=10)\n",
    "del risk_docs\n",
    "del mda_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train class membership = (0,0.925) and (1,0.075)\n",
      "Test class membership = (0,0.921) and (1,0.079)\n",
      "Using class weights:\n",
      "{0: 1.0811661073825503, 1: 13.320413436692506}\n"
     ]
    }
   ],
   "source": [
    "train_values = data_train[\"is_dps_cut\"].value_counts() / sum(data_train[\"is_dps_cut\"].value_counts())\n",
    "test_values = data_test[\"is_dps_cut\"].value_counts() / sum(data_test[\"is_dps_cut\"].value_counts())\n",
    "class_weight = {0: 1.0 / train_values[0], 1: 1.0 / train_values[1]}\n",
    "print(\"Train class membership = (0,{:.3f}) and (1,{:.3f})\".format(train_values[0], train_values[1]))\n",
    "print(\"Test class membership = (0,{:.3f}) and (1,{:.3f})\".format(test_values[0], test_values[1]))\n",
    "print(\"Using class weights:\")\n",
    "print(class_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier(random_state=5, warm_start=True, n_jobs=-1, verbose=1, class_weight=class_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num topics per sentence: 3\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "window = 1: Only one topics\n",
    "window = 5: Two topics\n",
    "window = 7: Three topics\n",
    "\"\"\"\n",
    "if window_size == 1:\n",
    "    #Yes, it is slower to use heapify for a single max element - easier implementation tho\n",
    "    num_topics = 1\n",
    "elif window_size == 5:\n",
    "    num_topics = 2\n",
    "elif window_size == 7:\n",
    "    num_topics = 3\n",
    "else:\n",
    "    print(\"ERROR\")\n",
    "print(\"Num topics per sentence: {}\".format(num_topics))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_weights(weights_arr, num_topics):\n",
    "    result = np.zeros((30,1))\n",
    "    if isinstance(weights_arr[0], list): # we have more than 1 set of weights\n",
    "        for sentence_grp in weights_arr:\n",
    "            top_topics = heapq.nlargest(num_topics, sentence_grp, key=lambda x: x[1])\n",
    "            for (idx_topic, weight) in top_topics:\n",
    "                result[idx_topic] += weight\n",
    "    else:\n",
    "        \"\"\"\n",
    "        Just a single set of weights -> Use it! Edge case for very short docs.\n",
    "        If we were to only use top x and normalize,\n",
    "        then it would seem like these documents strongly related to a topic\n",
    "        -> This isn't actually hit ever I dont think\n",
    "        \"\"\" \n",
    "        print(\"Single\")\n",
    "        result = np.array([topic_weight[1] for topic_weight in weights_arr], dtype=np.float64)[:,None] # grab only the weight\n",
    "    return result / np.linalg.norm(result, ord=1) # Normalize before returning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For training we have 5155 documents\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 48 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done 100 out of 100 | elapsed:    0.3s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(class_weight={0: 1.0811661073825503,\n",
       "                                     1: 13.320413436692506},\n",
       "                       n_jobs=-1, random_state=5, verbose=1, warm_start=True)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"For training we have {} documents\".format(len(data_train)))\n",
    "\n",
    "risk_docs = [sentence_grp for doc in data_train[\"item1a_risk\"].to_list() for sentence_grp in doc]\n",
    "risk_corpus = [risk_dict.doc2bow(doc) for doc in risk_docs]\n",
    "del risk_docs\n",
    "\n",
    "mda_docs = [sentence_grp for doc in data_train[\"item7_mda\"].to_list() for sentence_grp in doc]\n",
    "mda_corpus = [mda_dict.doc2bow(doc) for doc in mda_docs]\n",
    "del mda_docs\n",
    "\n",
    "\n",
    "documents_weights = np.zeros((len(data_train), 60))\n",
    "idx_risk = 0\n",
    "idx_mda = 0\n",
    "for idx_slice in range(len(data_train)):\n",
    "    row = data_train.iloc[idx_slice]\n",
    "    n_risk = len(row[\"item1a_risk\"])\n",
    "    n_mda = len(row[\"item7_mda\"])\n",
    "\n",
    "    row_risk_results = [item for item in lda_risk[risk_corpus[idx_risk:idx_risk+n_risk]]]\n",
    "    row_mda_results = [item for item in lda_mda[mda_corpus[idx_mda:idx_mda+n_mda]]]\n",
    "\n",
    "    weights_risk = parse_weights(row_risk_results, num_topics)\n",
    "    weights_mda = parse_weights(row_mda_results, num_topics)\n",
    "    weights = np.concatenate((weights_risk, weights_mda), axis=0)\n",
    "\n",
    "    documents_weights[idx_slice,:] = weights.squeeze()\n",
    "\n",
    "    idx_risk += n_risk\n",
    "    idx_mda += n_mda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf.fit(X=documents_weights,y=data_train[\"is_dps_cut\"].to_list())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"For testing we have {} documents\".format(len(data_test)))\n",
    "test_risk_docs = [sentence_grp for doc in data_test[\"item1a_risk\"].to_list() for sentence_grp in doc]\n",
    "test_risk_corpus = [risk_dict.doc2bow(doc) for doc in test_risk_docs]\n",
    "\n",
    "test_mda_docs = [sentence_grp for doc in data_test[\"item7_mda\"].to_list() for sentence_grp in doc]\n",
    "test_mda_corpus = [mda_dict.doc2bow(doc) for doc in test_mda_docs]\n",
    "\n",
    "del test_risk_docs\n",
    "del test_mda_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find testing features\n",
    "test_documents_weights = np.zeros((len(data_test), 60))\n",
    "idx_risk = 0\n",
    "idx_mda = 0\n",
    "for idx_slice in range(len(data_test)):\n",
    "    row = data_test.iloc[idx_slice]\n",
    "    n_risk = len(row[\"item1a_risk\"])\n",
    "    n_mda = len(row[\"item7_mda\"])\n",
    "    \n",
    "    row_risk_results = [item for item in lda_risk[test_risk_corpus[idx_risk:idx_risk+n_risk]]]\n",
    "    row_mda_results = [item for item in lda_mda[test_mda_corpus[idx_mda:idx_mda+n_mda]]]\n",
    "    \n",
    "    weights_risk = parse_weights(row_risk_results, num_topics)\n",
    "    weights_mda = parse_weights(row_mda_results, num_topics)\n",
    "    weights = np.concatenate((weights_risk, weights_mda), axis=0)\n",
    "    \n",
    "    test_documents_weights[idx_slice,:] = weights.squeeze()\n",
    "    \n",
    "    idx_risk += n_risk\n",
    "    idx_mda += n_mda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=48)]: Using backend ThreadingBackend with 48 concurrent workers.\n",
      "[Parallel(n_jobs=48)]: Done 100 out of 100 | elapsed:    0.1s finished\n"
     ]
    }
   ],
   "source": [
    "y_pred = rf.predict(test_documents_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_actual = data_test[\"is_dps_cut\"].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9212\n",
      "Precision: 0.0000\n",
      "Recall: 0.0000\n",
      "F1-score: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hshukla/anaconda3/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "accuracy = accuracy_score(y_actual, y_pred)\n",
    "precision = precision_score(y_actual, y_pred)\n",
    "recall = recall_score(y_actual, y_pred)\n",
    "f1 = f1_score(y_actual, y_pred)\n",
    "print(\"Accuracy: {:.4f}\".format(accuracy))\n",
    "print(\"Precision: {:.4f}\".format(precision))\n",
    "print(\"Recall: {:.4f}\".format(recall))\n",
    "print(\"F1-score: {:.4f}\".format(f1))'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing to disk\n",
      "Writing training\n",
      "Writing testing\n",
      "Writing forest\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['/mnt/nfs/scratch1/hshukla/prediction_results/rf_sentencelda_2012_2015_2016_7.pkl']"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_columns = [\"risk_topic_\" + str(i) for i in range(30)] + [\"mda_topic_\" + str(i) for i in range(30)]\n",
    "\n",
    "# Copy and add training features\n",
    "training_data = data_train.copy().reset_index()\n",
    "training_features = pd.DataFrame(data=documents_weights, columns=feature_columns)\n",
    "training_data_output = pd.concat([training_data, training_features], axis=1)\n",
    "\n",
    "# Copy and add predictions + training features\n",
    "testing_data = data_test.copy().reset_index()\n",
    "testing_data[\"dps_cut_prediction\"] = y_pred\n",
    "testing_features =  pd.DataFrame(data=test_documents_weights, columns=feature_columns)\n",
    "testing_data_output = pd.concat([testing_data, testing_features], axis=1)\n",
    "\n",
    "# Write to disk\n",
    "print(\"Writing to disk\")\n",
    "print(\"Writing training\")\n",
    "training_data_output.to_csv(output_folder + \"training_{}_{}_{}_{}.csv\".format(start_year, end_year, predict_year, window_size))\n",
    "print(\"Writing testing\")\n",
    "testing_data_output.to_csv(output_folder + \"testing_{}_{}_{}_{}.csv\".format(start_year, end_year, predict_year, window_size))\n",
    "print(\"Writing forest\")\n",
    "joblib.dump(rf, output_folder + 'rf_sentencelda_{}_{}_{}_{}.pkl'.format(start_year, end_year, predict_year, window_size))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
