{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from gensim.models.ldamulticore import LdaMulticore\n",
    "\n",
    "from gensim.models import Phrases\n",
    "from gensim.parsing.preprocessing import preprocess_string, remove_stopwords, strip_numeric, strip_punctuation, strip_short, stem_text\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim import corpora\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.corpora import Dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom Year Definition\n",
    "Modify these variables for your needs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "FILE_PATH = '../Files/gensim/'\n",
    "\n",
    "# Selected years\n",
    "SELECTED_YEARS = [2012, 2013]\n",
    "\n",
    "# Add bigrams and trigrams\n",
    "ADD_BIGRAMS = True\n",
    "\n",
    "# Only add bigrams that appear BIGRAMS_MIN_COUNT times or more\n",
    "BIGRAMS_MIN_COUNT = 20\n",
    "\n",
    "# Filter out words that occur in less than FILTER_NO_ABOVE documents\n",
    "FILTER_NO_BELOW = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Alan\\Documents\\Projects School\\696DS\\voya_project_spring21\\LDA\\util.ipynb:19: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  \"    relevant_cols = [\\\"PERMID\\\", \\\"CIK\\\", \\\"Ticker\\\", \\\"year\\\", \\\"FilingDate\\\", \\\"company_name\\\", \\\"Dividend Payer\\\", \\\"DPS growth\\\", \\\"DPS cut\\\", \\\"zEnvironmental\\\", \\\"dEnvironmental\\\", \\\"sector\\\"]\\n\",\n"
     ]
    }
   ],
   "source": [
    "from ipynb.fs.full.util import load_data\n",
    "X, y = load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_by_year = X[X.year.isin(SELECTED_YEARS)]\n",
    "items = {\n",
    "    'item1a': filtered_by_year['item1a_risk'],\n",
    "    'item7': filtered_by_year['item7_mda']\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got 5119 documents\n"
     ]
    }
   ],
   "source": [
    "print(f'Got {filtered_by_year.shape[0]} documents')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Alan\\Documents\\Projects School\\696DS\\voya_project_spring21\\venv\\lib\\site-packages\\pandas\\core\\indexing.py:1637: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self._setitem_single_block(indexer, value, name)\n",
      "C:\\Users\\Alan\\Documents\\Projects School\\696DS\\voya_project_spring21\\venv\\lib\\site-packages\\pandas\\core\\indexing.py:692: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  iloc._setitem_with_indexer(indexer, value, self.name)\n"
     ]
    }
   ],
   "source": [
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "for item in items:\n",
    "    docs = items[item]\n",
    "    for idx in range(len(docs)):\n",
    "        docs.iloc[idx] = docs.iloc[idx].lower()  # Convert to lowercase.\n",
    "        docs.iloc[idx] = tokenizer.tokenize(docs.iloc[idx])  # Split into words.\n",
    "        docs.iloc[idx] = docs.iloc[idx][4:] # Remove first 4 words\n",
    "    \n",
    "    # Remove numbers, but not words that contain numbers.\n",
    "    docs = [[token for token in doc if not token.isnumeric()] for doc in docs]\n",
    "\n",
    "    # Remove words that are only one character.\n",
    "    docs = [[token for token in doc if len(token) > 1] for doc in docs]\n",
    "    \n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    docs = [[lemmatizer.lemmatize(token) for token in doc] for doc in docs]\n",
    "    \n",
    "    # Add bigrams and trigrams to docs (only ones that appear BIGRAMS_MIN_COUNT times or more).\n",
    "    if ADD_BIGRAMS:\n",
    "        bigram = Phrases(docs, min_count=BIGRAMS_MIN_COUNT)\n",
    "        for idx in range(len(docs)):\n",
    "            for token in bigram[docs[idx]]:\n",
    "                if '_' in token:\n",
    "                    # Token is a bigram, add to document.\n",
    "                    docs[idx].append(token)\n",
    "                \n",
    "    items[item] = docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionaries = {}\n",
    "for item in items:\n",
    "    dictionaries[item] = Dictionary(items[item])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dictionary in dictionaries.values():\n",
    "    # Filter out words that occur in less than FILTER_NO_BELOW documents.\n",
    "    dictionary.filter_extremes(no_below=FILTER_NO_BELOW)\n",
    "    \n",
    "#     dictionary.filter_extremes(no_below=20, no_above=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = {}\n",
    "for item in items:\n",
    "    corpus[item] = [dictionaries[item].doc2bow(doc) for doc in items[item]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "item1a:\n",
      "\tNumber of unique tokens: 12265\n",
      "\tNumber of documents: 5119\n",
      "item7:\n",
      "\tNumber of unique tokens: 12891\n",
      "\tNumber of documents: 5119\n"
     ]
    }
   ],
   "source": [
    "for item in items:\n",
    "    print(item + ':')\n",
    "    print('\\tNumber of unique tokens: %d' % len(dictionaries[item]))\n",
    "    print('\\tNumber of documents: %d' % len(corpus[item]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "id2word = {}\n",
    "for item in items:\n",
    "    temp = dictionaries[item][0] # Initialize id2token mappings\n",
    "    id2word[item] = dictionaries[item].id2token"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write output to file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "base_name = str(SELECTED_YEARS[0])\n",
    "if len(SELECTED_YEARS) > 1: base_name += f'-{SELECTED_YEARS[-1]}'\n",
    "base_name += '_{}_{}.pkl'\n",
    "\n",
    "str_mapping = {\n",
    "    'corpus': corpus,\n",
    "    'id2word': id2word\n",
    "}\n",
    "\n",
    "for item in items:\n",
    "    for obj in str_mapping:\n",
    "        with open(FILE_PATH+base_name.format(item, obj), 'wb') as file:\n",
    "            pickle.dump(str_mapping[obj][item], file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pickle.dumps(corpus['item1a'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_id2word = pickle.dumps(id2word['item1a'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
