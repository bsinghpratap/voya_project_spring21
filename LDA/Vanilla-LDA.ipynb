{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from gensim.models.ldamulticore import LdaMulticore\n",
    "\n",
    "from gensim.models import Phrases\n",
    "from gensim.parsing.preprocessing import preprocess_string, remove_stopwords, strip_numeric, strip_punctuation, strip_short, stem_text\n",
    "from gensim.test.utils import common_corpus\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Alan\\Documents\\Projects School\\696DS\\voya_project_spring21\\LDA\\util.ipynb:19: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  \"    relevant_cols = [\\\"PERMID\\\", \\\"CIK\\\", \\\"Ticker\\\", \\\"year\\\", \\\"FilingDate\\\", \\\"company_name\\\", \\\"Dividend Payer\\\", \\\"DPS growth\\\", \\\"DPS cut\\\", \\\"zEnvironmental\\\", \\\"dEnvironmental\\\", \\\"sector\\\"]\\n\",\n"
     ]
    }
   ],
   "source": [
    "from ipynb.fs.full.util import load_data, preprocess_for_gensim\n",
    "X, y = load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim import corpora\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.corpora import Dictionary\n",
    "\n",
    "# def preprocess_for_gensim(data):\n",
    "#     for section in ('item1a_risk', 'item7_mda'):\n",
    "#         doc_list = data[section]\n",
    "#         doc_tokenized = [simple_preprocess(doc) for doc in doc_list]\n",
    "#         dictionary = corpora.Dictionary()\n",
    "#         bow_corpus = [dictionary.doc2bow(doc, allow_update=True) for doc in doc_tokenized]\n",
    "#         data[section+'_bow'] = bow_corpus\n",
    "    \n",
    "#     return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X = preprocess_for_gensim(X)\n",
    "# X.to_csv('../../Files/processed_data_bow.csv')\n",
    "# X, y = load_data(bow=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "items = {\n",
    "    'item1a': X[ (X.year >= 2012) & (X.year <= 2014) ]['item1a_risk'],\n",
    "    'item7': X[ (X.year >= 2012) & (X.year <= 2014) ]['item7_mda']\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Alan\\Documents\\Projects School\\696DS\\voya_project_spring21\\venv\\lib\\site-packages\\pandas\\core\\indexing.py:692: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  iloc._setitem_with_indexer(indexer, value, self.name)\n",
      "C:\\Users\\Alan\\Documents\\Projects School\\696DS\\voya_project_spring21\\venv\\lib\\site-packages\\pandas\\core\\indexing.py:1637: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self._setitem_single_block(indexer, value, name)\n"
     ]
    }
   ],
   "source": [
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "for item in items:\n",
    "    docs = items[item]\n",
    "    for idx in range(len(docs)):\n",
    "        docs.iloc[idx] = docs.iloc[idx].lower()  # Convert to lowercase.\n",
    "        docs.iloc[idx] = tokenizer.tokenize(docs.iloc[idx])  # Split into words.\n",
    "        docs.iloc[idx] = docs.iloc[idx][4:] # Remove first 4 words\n",
    "    \n",
    "    # Remove numbers, but not words that contain numbers.\n",
    "    docs = [[token for token in doc if not token.isnumeric()] for doc in docs]\n",
    "\n",
    "    # Remove words that are only one character.\n",
    "    docs = [[token for token in doc if len(token) > 1] for doc in docs]\n",
    "    \n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    docs = [[lemmatizer.lemmatize(token) for token in doc] for doc in docs]\n",
    "    \n",
    "    # Add bigrams and trigrams to docs (only ones that appear 20 times or more).\n",
    "#     bigram = Phrases(docs, min_count=20)\n",
    "#     for idx in range(len(docs)):\n",
    "#         for token in bigram[docs[idx]]:\n",
    "#             if '_' in token:\n",
    "#                 # Token is a bigram, add to document.\n",
    "#                 docs[idx].append(token)\n",
    "                \n",
    "    items[item] = docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionaries = {}\n",
    "for item in items:\n",
    "    dictionaries[item] = Dictionary(items[item])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dictionary in dictionaries.values():\n",
    "    # Filter out words that occur less than 20 documents, or more than 10% of the documents.\n",
    "    dictionary.filter_extremes(no_below=20, no_above=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = {}\n",
    "for item in items:\n",
    "    corpus[item] = [dictionaries[item].doc2bow(doc) for doc in items[item]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "item1a:\n",
      "\tNumber of unique tokens: 8423\n",
      "\tNumber of documents: 7662\n",
      "item7:\n",
      "\tNumber of unique tokens: 10005\n",
      "\tNumber of documents: 7662\n"
     ]
    }
   ],
   "source": [
    "for item in items:\n",
    "    print(item + ':')\n",
    "    print('\\tNumber of unique tokens: %d' % len(dictionaries[item]))\n",
    "    print('\\tNumber of documents: %d' % len(corpus[item]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    'num_topics': 30,\n",
    "    'chunksize': 2000,\n",
    "    'passes': 20,\n",
    "    'iterations': 400,\n",
    "    'eval_every': None,\n",
    "    'alpha': 'symmetric',\n",
    "    'eta': 'auto'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "id2word = {}\n",
    "for item in items:\n",
    "    temp = dictionaries[item][0] # Initialize id2token mappings\n",
    "    id2word[item] = dictionaries[item].id2token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8423"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# for item in items:\n",
    "#     _id2word = id2word[item]\n",
    "#     items[item].append(_id2word[len(_id2word)-1])\n",
    "# dictionaries['item1a'].id2token\n",
    "# temp = dictionaries['item1a'][0]\n",
    "# len(dictionaries['item1a'].id2token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {}\n",
    "for item in items:\n",
    "    models[item] = LdaMulticore(\n",
    "        corpus=corpus[item],\n",
    "        id2word=id2word[item],\n",
    "        workers=32,\n",
    "        **params\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "# models['item1a'].print_topics(num_topics=5, num_words=5)\n",
    "# models['item7'].print_topics(num_topics=5, num_words=5)\n",
    "# print(models['item1a'].show_topics(num_topics=5, num_words=5, formatted=True))\n",
    "\n",
    "# Look at things your throwing out in filtering\n",
    "# Look at strange occurences \"duke\"\n",
    "# Look at total perecent of words that are made up\n",
    "# Split by sector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "for doc_idx in range(item1a.shape[0]):\n",
    "    result_doc = {}\n",
    "    for item in items:\n",
    "        lda_model = models[item]\n",
    "        scores = []\n",
    "        topics = []\n",
    "        for index, score in sorted(lda_model[corpus[item][doc_idx]], key=lambda tup: -1*tup[1]):\n",
    "#             print (\"Score: {}\\t Topic ID: {} Topic: {}\".format(score, index, lda_model.print_topic(index, 10)))\n",
    "            topics.append(dictionaries[item][index])\n",
    "            scores.append(score)\n",
    "        result_doc[item] = (topics, scores)\n",
    "    results.append(result_doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "item1a:\n",
      "[0.03333334 0.03333334 0.03333334 0.03333334 0.03333334 0.03333334\n",
      " 0.03333334 0.03333334 0.03333334 0.03333334 0.03333334 0.03333334\n",
      " 0.03333334 0.03333334 0.03333334 0.03333334 0.03333334 0.03333334\n",
      " 0.03333334 0.03333334 0.03333334 0.03333334 0.03333334 0.03333334\n",
      " 0.03333334 0.03333334 0.03333334 0.03333334 0.03333334 0.03333334]\n",
      "[0.07455198 0.47855908 0.11533958 ... 0.04148679 0.0340654  0.03422894]\n",
      "item7:\n",
      "[0.03333334 0.03333334 0.03333334 0.03333334 0.03333334 0.03333334\n",
      " 0.03333334 0.03333334 0.03333334 0.03333334 0.03333334 0.03333334\n",
      " 0.03333334 0.03333334 0.03333334 0.03333334 0.03333334 0.03333334\n",
      " 0.03333334 0.03333334 0.03333334 0.03333334 0.03333334 0.03333334\n",
      " 0.03333334 0.03333334 0.03333334 0.03333334 0.03333334 0.03333334]\n",
      "[0.18083873 0.05202858 0.6822381  ... 0.08863842 0.05057403 0.06306964]\n"
     ]
    }
   ],
   "source": [
    "for item in items:\n",
    "    lda_model = models[item]\n",
    "    print(f'{item}:')\n",
    "    print(lda_model.alpha)\n",
    "    print(lda_model.eta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.02, 0.02, 0.02, ..., 0.02, 0.02, 0.02], dtype=float32)"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda_model.eta"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
